{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirtiibm/False-Data-injection/blob/Root-Directories/Water_Systems_Attack_Detection_FDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-eFAco_lCKL"
      },
      "outputs": [],
      "source": [
        "# numpy library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "\n",
        "# keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, load_model\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "\n",
        "# sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# json\n",
        "import json\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# sklearn library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# others\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azaIOT6wleDF",
        "outputId": "d11f261a-8fb0-4835-d06e-6b8201b7697f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset03.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8b8da2eaf375>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import dataset from .csv to dataframe type (df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset03_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'dataset03.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset04_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'dataset04.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'test_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset03.csv'"
          ]
        }
      ],
      "source": [
        "# import dataset from .csv to dataframe type (df)\n",
        "dataset03_df = pd.read_csv(r'dataset03.csv')\n",
        "dataset04_df = pd.read_csv(r'dataset04.csv')\n",
        "test_dataset_df = pd.read_csv(r'test_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbcLW2HemNYd"
      },
      "outputs": [],
      "source": [
        "def configure_plotly_browser_state(plotly_js_url='https://cdn.plot.ly/plotly-latest.min.js'):\n",
        "    import IPython\n",
        "\n",
        "    # Define JavaScript configuration\n",
        "    js_config = f'''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({{\n",
        "            paths: {{\n",
        "              base: '/static/base',\n",
        "              plotly: '{plotly_js_url}?noext',\n",
        "            }},\n",
        "          }});\n",
        "        </script>\n",
        "    '''\n",
        "\n",
        "    # Display JavaScript configuration\n",
        "    display(IPython.core.display.HTML(js_config))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWhwEPahm7pz"
      },
      "outputs": [],
      "source": [
        "# get different color maps for all features in the dataset\n",
        "pal = list(sns.color_palette(palette='Spectral', n_colors=dataset03_df.shape[1]).as_hex())+list(sns.color_palette(palette='Spectral', n_colors=dataset04_df.shape[1]).as_hex())\n",
        "len(pal)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Presentation\n",
        "\n",
        "### **Process Training Dataset** (Dataset03)\n"
      ],
      "metadata": {
        "id": "9IL-eyFfn4-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ePSizIZncYP"
      },
      "outputs": [],
      "source": [
        "# import the orginal dataset. df is \"dataframe\"\n",
        "df_train_orig = pd.read_csv(r'dataset03.csv')\n",
        "# get dates and columns with sensor readings\n",
        "dates_train = df_train_orig['DATETIME']\n",
        "sensor_cols = [col for col in df_train_orig.columns if col not in ['DATETIME','ATT_FLAG']]\n",
        "# scale sensor data\n",
        "scaler = MinMaxScaler()\n",
        "X = pd.DataFrame(index = df_train_orig.index, columns = sensor_cols, data = scaler.fit_transform(df_train_orig[sensor_cols]))\n",
        "\n",
        "# split into training and validation\n",
        "X1, X2, _, _  = train_test_split(X, X, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Process 2 Testing Datasets** (Dataset04 and Test Dataset)"
      ],
      "metadata": {
        "id": "UiJUz-zhoc-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset with attacks\n",
        "df_test_01 = pd.read_csv(r'dataset04.csv')\n",
        "df_test_02 = pd.read_csv(r'test_dataset.csv')\n",
        "\n",
        "# scale datasets\n",
        "X3 = pd.DataFrame(index = df_test_01.index, columns = sensor_cols,\n",
        "                  data = scaler.transform(df_test_01[sensor_cols]))\n",
        "X4 = pd.DataFrame(index = df_test_02.index, columns = sensor_cols,\n",
        "                  data = scaler.transform(df_test_02[sensor_cols]))\n",
        "\n",
        "# get targets\n",
        "Y3 = df_test_01['ATT_FLAG']\n",
        "Y4 = df_test_02['ATT_FLAG']"
      ],
      "metadata": {
        "id": "j3iaQhepoisv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes\n",
        "class AutoEncoder(object):\n",
        "    \"\"\" Keras-based AutoEncoder (AE) class used for event detection.\n",
        "        Attributes:\n",
        "        params: dictionary with parameters defining the AE structure,\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\" Class constructor, stores parameters and initialize AE Keras model. \"\"\"\n",
        "\n",
        "        # Default parameters values. If nI is not given, the code will crash later.\n",
        "        params = {\n",
        "            'nI': None,\n",
        "            'nH': 3,\n",
        "            'cf': 1,\n",
        "            'activation' : 'tanh',\n",
        "            'optimizer' : None,\n",
        "            'verbose' : 0\n",
        "            }\n",
        "\n",
        "        for key,item in kwargs.items():\n",
        "            params[key] = item\n",
        "\n",
        "        self.params = params\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\" Creates Keras AE model.\n",
        "            The model has nI inputs, nH hidden layers in the encoder (and decoder)\n",
        "            and cf compression factor. The compression factor is the ratio between\n",
        "            the number of inputs and the innermost hidden layer which stands between\n",
        "            the encoder and the decoder. The size of the hidden layers between the\n",
        "            input (output) layer and the innermost layer decreases (increase) linearly\n",
        "            according to the cg.\n",
        "        \"\"\"\n",
        "\n",
        "        # retrieve params\n",
        "        nI = self.params['nI'] # number of inputs\n",
        "        nH = self.params['nH'] # number of hidden layers in encoder (decoder)\n",
        "        cf = self.params['cf'] # compression factor\n",
        "        activation = self.params['activation'] # autoencoder activation fWeunction\n",
        "        optimizer = self.params['optimizer'] # Keras optimizer\n",
        "        verbose = self.params['verbose'] # echo on screen\n",
        "\n",
        "        # get number/size of hidden layers for encoder and decoder\n",
        "        temp = np.linspace(nI,nI/cf,nH + 1).astype(int)\n",
        "        nH_enc = temp[1:]\n",
        "        nH_dec = temp[:-1][::-1]\n",
        "\n",
        "        # input layer placeholder\n",
        "        input_layer = Input(shape=(nI,))\n",
        "\n",
        "        # build encoder\n",
        "        for i, layer_size in enumerate(nH_enc):\n",
        "            if i == 0:\n",
        "                # first hidden layer\n",
        "                encoder = Dense(layer_size, activation=activation)(input_layer)\n",
        "            else:\n",
        "                # other hidden layers\n",
        "                encoder = Dense(layer_size, activation=activation)(encoder)\n",
        "\n",
        "        # build decoder\n",
        "        for i, layer_size in enumerate(nH_dec):\n",
        "            if i == 0:\n",
        "                # first hidden layer\n",
        "                decoder = Dense(layer_size, activation=activation)(encoder)\n",
        "            else:\n",
        "                # other hidden layers\n",
        "                decoder = Dense(layer_size, activation=activation)(decoder)\n",
        "\n",
        "        # create autoencoder\n",
        "        autoencoder = Model(input_layer, decoder)\n",
        "        if optimizer == None:\n",
        "            optimizer = tf.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "        # print autoencoder specs\n",
        "        if verbose > 0:\n",
        "            print('Created autoencoder with structure:');\n",
        "            print(', '.join('layer_{}: {}'.format(v, i) for v, i in enumerate(np.hstack([nI,nH_enc,nH_dec]))))\n",
        "\n",
        "        # compile and return model\n",
        "        autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "        return autoencoder\n",
        "\n",
        "    def train(self, x, **train_params):\n",
        "        \"\"\" Train autoencoder,\n",
        "            x: inputs (inputs == targets, AE are self-supervised ANN).\n",
        "        \"\"\"\n",
        "        if self.params['verbose']:\n",
        "            if self.ann == None:\n",
        "                print('Creating model.')\n",
        "                self.create_model()\n",
        "        self.ann.fit(x, x, **train_params)\n",
        "\n",
        "\n",
        "    def predict(self, x, test_params={}):\n",
        "        \"\"\" Yields reconstruction error for all inputs,\n",
        "            x: inputs.\n",
        "        \"\"\"\n",
        "        return self.ann.predict(x, **test_params)\n",
        "\n",
        "class AEED(AutoEncoder):\n",
        "    \"\"\" This class extends the AutoEncoder class to include event detection\n",
        "        functionalities.\n",
        "    \"\"\"\n",
        "    def initialize(self):\n",
        "        \"\"\" Create the underlying Keras model. \"\"\"\n",
        "        self.ann = self.create_model()\n",
        "\n",
        "    def predict(self, x, **keras_params):\n",
        "        \"\"\" Predict with autoencoder. \"\"\"\n",
        "        preds = pd.DataFrame(index=x.index,columns=x.columns,\n",
        "                                            data=super(AEED, self).predict(x.values,keras_params))\n",
        "        errors = (x-preds)**2\n",
        "        return preds, errors\n",
        "\n",
        "    def detect(self, x, theta, window = 1, average=False, sys_theta = 0, **keras_params):\n",
        "        \"\"\" Detection performed based on (smoothed) reconstruction errors.\n",
        "            x = inputs,\n",
        "            theta = threshold, attack flagged if reconstruction error > threshold,\n",
        "            window = length of the smoothing window (default = 1 timestep, i.e. no smoothing),\n",
        "            average = boolean (default = False), if True the detection is performed\n",
        "                on the average reconstruction error across all outputs,\n",
        "            keras_params = parameters for the Keras-based AE prediction.\n",
        "        \"\"\"\n",
        "        #        preds = super(AEED, self).predict(x,keras_params)\n",
        "        preds, temp = self.predict(x, **keras_params)\n",
        "        temp = (x-preds)**2\n",
        "        if average:\n",
        "            errors = temp.mean(axis=1).rolling(window=window).mean()\n",
        "            detection = errors > theta\n",
        "        else:\n",
        "            errors = temp.rolling(window=window).mean()\n",
        "            detection = errors.apply(lambda x: x>np.max(theta.name, sys_theta))\n",
        "\n",
        "        return detection, errors\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\" Save AEED model.\n",
        "            AEED parameters saved in a .json, while Keras model is stored in .h5 .\n",
        "        \"\"\"\n",
        "        # parameters\n",
        "        with open(filename+'.json', 'w') as fp:\n",
        "            json.dump(self.params, fp)\n",
        "        # keras model\n",
        "        self.ann.save(filename+'.h5')\n",
        "        # echo\n",
        "        print('Saved AEED parameters to {0}.\\nKeras model saved to {1}'.format(filename+'.json', filename+'.h5'))\n",
        "\n",
        "\n",
        "# functions\n",
        "def load_AEED(params_filename, model_filename):\n",
        "    \"\"\" Load stored AEED. \"\"\"\n",
        "    # load params and create AEED\n",
        "    with open(params_filename) as fd:\n",
        "        params = json.load(fd)\n",
        "    aeed = AEED(**params)\n",
        "\n",
        "    # load keras model\n",
        "    aeed.ann = load_model(model_filename)\n",
        "    return aeed"
      ],
      "metadata": {
        "id": "Hcynpxx2ouSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model parameters\n",
        "params = {\n",
        "    'nI' : X.shape[1],\n",
        "    'nH' : 3,\n",
        "    'cf' : 2.5,\n",
        "    'activation' : 'tanh',\n",
        "    'verbose' : 1,\n",
        "}\n",
        "\n",
        "# create AutoEncoder for Event Detection (AEED)\n",
        "autoencoder = AEED(**params)\n",
        "autoencoder.initialize()"
      ],
      "metadata": {
        "id": "X8LT4O8MpOrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train models with early stopping and reduction of learning rate on plateau\n",
        "earlyStopping= EarlyStopping(monitor='val_loss', patience=3, verbose=0,  min_delta=1e-4, mode='auto')\n",
        "lr_reduced = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=0, epsilon=1e-4, mode='min')\n",
        "\n",
        "# train autoencoder\n",
        "autoencoder.train(X1.values,\n",
        "            epochs=500,                              # train for the maximum times of 500 times\n",
        "            batch_size=40,                           # learning-material \"chunks\" size\n",
        "            shuffle=False,                           # don't mix the material randomly\n",
        "            callbacks = [earlyStopping, lr_reduced], # training optimization methods\n",
        "            verbose = 2,                             # provide me 1 line of results per epoch\n",
        "            validation_data=(X2.values, X2.values))  # after every train, test yourself"
      ],
      "metadata": {
        "id": "JBUuZE68pZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assess detection\n",
        "def compute_scores(Y,Yhat):\n",
        "    return [accuracy_score(Y,Yhat),f1_score(Y,Yhat),precision_score(Y,Yhat),recall_score(Y,Yhat)]\n",
        "    # get validation reconstruction errors\n",
        "_, validation_errors = autoencoder.predict(X2)\n",
        "\n",
        "# set treshold as quantile of average reconstruction error\n",
        "theta = validation_errors.mean(axis = 1).quantile(0.995)\n",
        "\n",
        "Yhat3, _ = autoencoder.detect(X3, theta = theta , window = 3, average=True)\n",
        "Yhat4, _ = autoencoder.detect(X4, theta = theta, window = 3, average=True)"
      ],
      "metadata": {
        "id": "mccWW5z1pqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(index = ['test dataset 01','test dataset 02'],\n",
        "                       columns = ['accuracy','f1_score','precision','recall'])\n",
        "results.loc['test dataset 01'] = compute_scores(Y3,Yhat3)\n",
        "results.loc['test dataset 02'] = compute_scores(Y4,Yhat4)\n",
        "\n",
        "print('Results:\\n')\n",
        "print(results)"
      ],
      "metadata": {
        "id": "eMW4jrc9qDuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot figures\n",
        "shade_of_gray = '0.75'\n",
        "f, axes = plt.subplots(2,figsize = (20,8))\n",
        "axes[0].plot(Yhat3, color = shade_of_gray, label = 'predicted state')\n",
        "axes[0].fill_between(Yhat3.index, Yhat3.values, where=Yhat3.values <=1, interpolate=True, color=shade_of_gray)\n",
        "axes[0].plot(Y3, color = 'r', alpha = 0.85, lw = 5, label = 'real state')\n",
        "axes[0].set_title('Detection trajectory on test dataset 01', fontsize = 14)\n",
        "axes[0].set_yticks([0,1])\n",
        "axes[0].set_yticklabels(['NO ATTACK','ATTACK'])\n",
        "axes[0].legend(fontsize = 12, loc = 2)\n",
        "\n",
        "axes[1].plot(Yhat4, color = shade_of_gray, label = 'predicted state')\n",
        "axes[1].fill_between(Yhat4.index, Yhat4.values, where=Yhat4.values <=1, interpolate=True, color=shade_of_gray)\n",
        "axes[1].plot(Y4, color = 'r', alpha = 0.75, lw = 5, label = 'real state')\n",
        "axes[1].set_title('Detection trajectory on test dataset 02', fontsize = 14)\n",
        "axes[1].set_yticks([0,1])\n",
        "axes[1].set_yticklabels(['NO ATTACK','ATTACK'])"
      ],
      "metadata": {
        "id": "1hehgcnQqEbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute confusion matrix for test dataset 01\n",
        "cm1 = confusion_matrix(Y3, Yhat3)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm1, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Dataset 01\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# Compute confusion matrix for test dataset 02\n",
        "cm2 = confusion_matrix(Y4, Yhat4)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Dataset 02\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# Define function to extract TP, FP, TN, FN from confusion matrix\n",
        "def extract_confusion_values(cm):\n",
        "    TN = cm[0, 0]\n",
        "    FP = cm[0, 1]\n",
        "    FN = cm[1, 0]\n",
        "    TP = cm[1, 1]\n",
        "    return TP, FP, TN, FN\n",
        "\n",
        "# Extract values for test dataset 01\n",
        "TP1, FP1, TN1, FN1 = extract_confusion_values(cm1)\n",
        "\n",
        "# Plot for test dataset 01\n",
        "plt.figure(figsize=(10, 6))\n",
        "barWidth = 0.3\n",
        "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "\n",
        "# Position of bars on x-axis\n",
        "r1 = np.arange(len(labels))\n",
        "\n",
        "# Make the plot\n",
        "plt.bar(r1, [TN1, FP1, FN1, TP1], color=['#2d7f5e', '#ff7f0e', '#d62728', '#9467bd'])\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('Metrics', fontweight='bold')\n",
        "plt.xticks([r for r in range(len(labels))], labels)\n",
        "plt.ylabel('Count', fontweight='bold')\n",
        "plt.title('Confusion Matrix Breakdown - Test Dataset 01')\n",
        "plt.show()\n",
        "\n",
        "# Extract values for test dataset 02\n",
        "TP2, FP2, TN2, FN2 = extract_confusion_values(cm2)\n",
        "\n",
        "# Plot for test dataset 02\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Make the plot\n",
        "plt.bar(r1, [TN2, FP2, FN2, TP2], color=['#2d7f5e', '#ff7f0e', '#d62728', '#9467bd'])\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('Metrics', fontweight='bold')\n",
        "plt.xticks([r for r in range(len(labels))], labels)\n",
        "plt.ylabel('Count', fontweight='bold')\n",
        "plt.title('Confusion Matrix Breakdown - Test Dataset 02')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gVjQpdr3qOhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.keras.models.save_model(model, 'model.pbtxt')\n",
        "converter = tensorflow.lite.TFliteConverter.from_keras_model(model = model)\n",
        "model_tflite = converter.convert()\n",
        "open(\"FDIattack.tflite\", \"wb\").write(model_tflite) # Removed extra indentation on this line"
      ],
      "metadata": {
        "id": "ajmNvV-S3rCM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcFB48slIUhcTz6uURCt8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}